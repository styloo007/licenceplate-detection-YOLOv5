{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrsaDfdVHzxt"
      },
      "source": [
        "# Custom Training with YOLOv5\n",
        "\n",
        "In this tutorial, we assemble a dataset and train a custom YOLOv5 model to recognize the objects in our dataset. To do so we will take the following steps:\n",
        "\n",
        "* Gather a dataset of images and label our dataset\n",
        "* Export our dataset to YOLOv5\n",
        "* Train YOLOv5 to recognize the objects in our dataset\n",
        "* Evaluate our YOLOv5 model's performance\n",
        "* Run test inference to view our model at work\n",
        "\n",
        "\n",
        "\n",
        "![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNveqeA1KXGy"
      },
      "source": [
        "# Step 1: Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTvDNSILZoN9",
        "outputId": "f428bb77-f230-4792-d248-336eb35c76a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 16556, done.\u001b[K\n",
            "remote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 16556 (delta 66), reused 116 (delta 47), pack-reused 16408\u001b[K\n",
            "Receiving objects: 100% (16556/16556), 15.18 MiB | 18.33 MiB/s, done.\n",
            "Resolving deltas: 100% (11331/11331), done.\n",
            "/content/yolov5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m716.0/716.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.2/70.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSetup complete. Using torch 2.1.0+cu121 (Tesla T4)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "Z5lsRnbkGEzy",
        "outputId": "1f24e16c-c6d3-44dc-8b3f-1609cf2fdfa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-2.2.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-2.2.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO patool: Extracting /content/testset.zip ...\n",
            "INFO:patool:Extracting /content/testset.zip ...\n",
            "INFO patool: running /usr/bin/7z x -o./Unpack_i3wctmbs -- /content/testset.zip\n",
            "INFO:patool:running /usr/bin/7z x -o./Unpack_i3wctmbs -- /content/testset.zip\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... /content/testset.zip extracted to `testset'.\n",
            "INFO:patool:... /content/testset.zip extracted to `testset'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'testset'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install patool\n",
        "import patoolib\n",
        "patoolib.extract_archive('/content/testset.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC4UeuTSTXqn",
        "outputId": "13f7ee0c-ad0c-4715-a555-83fe9ecfc70c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Directory: /content/yolov5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "print(\"Current Directory:\", current_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHL42v-XF67x"
      },
      "outputs": [],
      "source": [
        "# Inference for Single Rider in a Frame\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load the model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', '/content/1500img.pt')\n",
        "\n",
        "# Load class names\n",
        "names = model.names\n",
        "\n",
        "# Define the folder containing images\n",
        "image_folder = \"/content/datasets/helmet-2/test/images\"  # Replace with your folder path\n",
        "\n",
        "# Iterate through images in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        print(image_path)\n",
        "        # Perform inference\n",
        "        results = model(image_path)\n",
        "        results.show()\n",
        "\n",
        "        # Check for nested bounding boxes and missing helmets\n",
        "        for i, result1 in enumerate(results.xyxy[0]):\n",
        "            x1_1, y1_1, x2_1, y2_1, conf1, cls1 = result1\n",
        "            class_name1 = names[int(cls1)]\n",
        "\n",
        "            if class_name1 == 'rider':\n",
        "                helmet_found = False\n",
        "                for j, result2 in enumerate(results.xyxy[0]):\n",
        "                    if i != j:\n",
        "                        x1_2, y1_2, x2_2, y2_2, conf2, cls2 = result2\n",
        "                        class_name2 = names[int(cls2)]\n",
        "                        if class_name2 == 'helmet' and x1_2 > x1_1 and y1_2 > y1_1 and x2_2 < x2_1 and y2_2 < y2_1:\n",
        "                            helmet_found = True\n",
        "                            break\n",
        "\n",
        "                if not helmet_found:\n",
        "                    print(f\"Image: {filename} - No helmet is detected inside the rider class at bounding box: {x1_1}, {y1_1}, {x2_1}, {y2_1}\")\n",
        "                    !python detect.py --weights /content/1500img.pt --source /content/datasets/helmet-2/{filename} --img 416 --classes 1 --save-crop\n",
        "            else:\n",
        "                for j, result2 in enumerate(results.xyxy[0]):\n",
        "                    if i != j:\n",
        "                        x1_2, y1_2, x2_2, y2_2, conf2, cls2 = result2\n",
        "                        class_name2 = names[int(cls2)]\n",
        "                        if class_name2 == 'plate' and x1_2 > x1_1 and y1_2 > y1_1 and x2_2 < x2_1 and y2_2 < y2_1:\n",
        "                            print(f\"Image: {filename} - Class {class_name2} is inside class {class_name1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pokokiU5M0A3"
      },
      "outputs": [],
      "source": [
        "# Inference for Multiple Riders in a Single Frame\n",
        "\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def is_inside(box1, box2):\n",
        "  \"\"\"\n",
        "  Checks if box1 is completely inside box2.\n",
        "\n",
        "  Args:\n",
        "      box1: A dictionary containing 'xmin', 'ymin', 'xmax', and 'ymax' keys.\n",
        "      box2: A dictionary containing 'xmin', 'ymin', 'xmax', and 'ymax' keys.\n",
        "\n",
        "  Returns:\n",
        "      True if box1 is completely inside box2, False otherwise.\n",
        "  \"\"\"\n",
        "  return (box1['xmin'] >= box2['xmin'] and\n",
        "          box1['ymin'] >= box2['ymin'] and\n",
        "          box1['xmax'] <= box2['xmax'] and\n",
        "          box1['ymax'] <= box2['ymax'])\n",
        "\n",
        "# Model loading (assuming you have the model loaded)\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/1500img.pt')\n",
        "\n",
        "# Define image folder path\n",
        "image_folder = \"/content/testset\"\n",
        "\n",
        "# Iterate through images in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "  if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "    # Create full image path\n",
        "    image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "    # Inference\n",
        "    results = model(image_path)\n",
        "    results.show()\n",
        "\n",
        "    # Get bounding boxes and class names\n",
        "    boxes = results.pandas().xyxy[0].to_dict('records')\n",
        "    class_names = results.pandas().names[0] if results.pandas().names else [f\"box_{i+1}\" for i in range(len(boxes))]\n",
        "\n",
        "    # Find rider and helmet bounding boxes\n",
        "    rider_boxes = [box for box in boxes if box['name'] == \"rider\"]\n",
        "    helmet_boxes = [box for box in boxes if box['name'] == \"helmet\"]\n",
        "    license_plate_boxes = [box for box in boxes if box['name'] == \"plate\"]\n",
        "\n",
        "    # Check for helmet inside each rider box\n",
        "    rider_count = 1\n",
        "    for rider_box in rider_boxes:\n",
        "      helmet_found = False\n",
        "      for helmet_box in helmet_boxes:\n",
        "        if is_inside(helmet_box, rider_box):\n",
        "          helmet_found = True\n",
        "          break\n",
        "\n",
        "      if not helmet_found:\n",
        "        print(f\"No helmet found for rider {rider_count} in image: {filename}\")\n",
        "\n",
        "        # If no helmet found, find license plate inside the rider's box and crop it\n",
        "        for plate_box in license_plate_boxes:\n",
        "          if is_inside(plate_box, rider_box):\n",
        "            # Crop the license plate from the image\n",
        "            img = Image.open(image_path)\n",
        "            plate_img = img.crop((plate_box['xmin'], plate_box['ymin'], plate_box['xmax'], plate_box['ymax']))\n",
        "            # Save cropped image with filename\n",
        "            plate_img.save(f\"/content/LP_CROPS/rider_{rider_count}_plate_{filename}_.jpg\")\n",
        "            print(f\"License plate cropped for rider {rider_count} in image: {filename}\")\n",
        "\n",
        "      rider_count += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7frIXcbuW8l"
      },
      "outputs": [],
      "source": [
        "!pip install easyocr\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import imutils\n",
        "import easyocr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNZfYLANgwxU",
        "outputId": "e8ae17e1-dcf9-42f8-a7ad-23f51be4327b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text from rider_1_plate_0077_jpg.rf.78fe62e83395d931f12a920d0ac16fcb.jpg_.jpg : MAZZB 4ZS31\n",
            "Text from rider_1_plate_ADB03TE193676294_jpg.rf.26a1de88139e056d686058dfe8150d3e.jpg_.jpg : 7L30O\n",
            "Text from rider_1_plate_ADB03TE193684491_jpg.rf.270cedd023118d817d90f97e393bcfd5.jpg_.jpg : ISO1ELL4AI\n",
            "Text from rider_1_plate_KBA01EC191105189_jpg.rf.4a635226e33e51eac11d822423ba9423.jpg_.jpg : TS 20 8565\n",
            "Text from rider_1_plate_ADB03TE193710917_jpg.rf.224f2d2c94455baeb6f33699577ed0ea.jpg_.jpg : LS O EJ?4\n",
            "Text from rider_2_plate_ADB05EC198629625_jpg.rf.eb970c0cbf801ca03dc2425327fc9332.jpg_.jpg : EIS OHE98\n",
            "Text from rider_2_plate_0872_jpg.rf.d3f5056ba923ff3fcba136354ebfefc8.jpg_.jpg : TS01 EA5415\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import easyocr\n",
        "\n",
        "# Path to the directory containing images\n",
        "image_directory = \"/content/LP_CROPS/\"\n",
        "\n",
        "# Initialize the EasyOCR reader\n",
        "reader = easyocr.Reader(['en'])  # You can specify languages here\n",
        "\n",
        "# Path to the output text file\n",
        "output_file = \"/content/extracted_text.txt\"\n",
        "\n",
        "# Open the output text file in write mode\n",
        "with open(output_file, \"w\") as f:\n",
        "    # Loop through each file in the directory\n",
        "    for filename in os.listdir(image_directory):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            # Construct the full path to the image file\n",
        "            image_path = os.path.join(image_directory, filename)\n",
        "\n",
        "            # Load the image\n",
        "            image = cv2.imread(image_path)\n",
        "\n",
        "            # Define the new size\n",
        "            new_width = 5 * image.shape[1]  # Double the width\n",
        "            new_height = 5 * image.shape[0]  # Double the height\n",
        "\n",
        "            # Upscale the image using bilinear interpolation\n",
        "            upscaled_img = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            # Perform OCR on the upscaled image\n",
        "            result = reader.readtext(upscaled_img)\n",
        "\n",
        "            # Extract and print the text\n",
        "            extracted = ' '.join([entry[1] for entry in result])\n",
        "            extracted_text = extracted.upper()\n",
        "            print(\"Text from\", filename, \":\", extracted_text)\n",
        "\n",
        "            # Write the extracted text to the output text file\n",
        "            f.write(f\"Text from {filename}: {extracted_text}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
